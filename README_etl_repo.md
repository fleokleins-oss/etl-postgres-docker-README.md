# ETL Pipeline - PostgreSQL + Docker

Pipeline ETL completo demonstrando extraÃ§Ã£o, transformaÃ§Ã£o e carga de dados usando Python, PostgreSQL e Docker.

## ğŸ¯ Objetivo

Demonstrar um pipeline de dados production-ready com:
- ExtraÃ§Ã£o de dados de mÃºltiplas fontes
- TransformaÃ§Ãµes SQL e Python
- ValidaÃ§Ã£o de qualidade de dados
- ContainerizaÃ§Ã£o com Docker
- Logs e monitoramento

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sources   â”‚ (APIs, CSVs, DBs)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Extract
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Staging    â”‚ (Raw data)
â”‚  PostgreSQL â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Transform
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Marts  â”‚ (Analytics-ready)
â”‚  PostgreSQL â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### PrÃ©-requisitos
- Docker e Docker Compose instalados
- Python 3.9+

### Executar o Pipeline

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/fleokleins-oss/etl-postgres-docker.git
cd etl-postgres-docker

# 2. Subir o banco de dados
docker-compose up -d

# 3. Instalar dependÃªncias Python
pip install -r requirements.txt

# 4. Executar o ETL
python src/etl.py
```

### Verificar Resultados

```bash
# Conectar ao PostgreSQL
docker exec -it etl-postgres psql -U postgres -d analytics

# Ver dados transformados
SELECT * FROM marts.daily_metrics LIMIT 10;
```

## ğŸ“ Estrutura do Projeto

```
etl-postgres-docker/
â”œâ”€â”€ docker-compose.yml       # Setup PostgreSQL + Adminer
â”œâ”€â”€ requirements.txt         # DependÃªncias Python
â”œâ”€â”€ README.md               # Este arquivo
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ etl.py             # Pipeline principal
â”‚   â”œâ”€â”€ extract.py         # MÃ³dulo de extraÃ§Ã£o
â”‚   â”œâ”€â”€ transform.py       # MÃ³dulo de transformaÃ§Ã£o
â”‚   â”œâ”€â”€ load.py            # MÃ³dulo de carga
â”‚   â””â”€â”€ config.py          # ConfiguraÃ§Ãµes
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ schema.sql         # DDL do banco
â”‚   â”œâ”€â”€ staging/           # Queries de staging
â”‚   â””â”€â”€ marts/             # Queries de marts
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_extract.py
    â”œâ”€â”€ test_transform.py
    â””â”€â”€ test_quality.py
```

## ğŸ”§ Componentes

### Extract
- LÃª dados de APIs pÃºblicas (exemplo: JSONPlaceholder)
- Suporta CSVs e bancos de dados externos
- Tratamento de erros e retry logic

### Transform
- Limpeza de dados (nulls, duplicatas, outliers)
- AgregaÃ§Ãµes e joins
- Enriquecimento de dados
- ValidaÃ§Ãµes de qualidade

### Load
- InserÃ§Ã£o incremental (upsert)
- Partition por data
- Ãndices otimizados
- Logs de execuÃ§Ã£o

## ğŸ“Š Dados de Exemplo

O pipeline processa dados de usuÃ¡rios e posts:

**Staging (Raw):**
```sql
staging.users      -- Dados brutos de usuÃ¡rios
staging.posts      -- Dados brutos de posts
```

**Marts (Analytics-ready):**
```sql
marts.daily_metrics       -- MÃ©tricas diÃ¡rias agregadas
marts.user_summary        -- Resumo por usuÃ¡rio
marts.post_analytics      -- Analytics de posts
```

## ğŸ§ª ValidaÃ§Ãµes de Qualidade

```python
# Exemplos de validaÃ§Ãµes implementadas
- not_null: campos obrigatÃ³rios
- unique: unicidade de IDs
- relationships: integridade referencial
- accepted_values: domÃ­nios vÃ¡lidos
- custom_sql: regras de negÃ³cio
```

## ğŸ“ˆ Monitoramento

O pipeline gera logs estruturados:

```json
{
  "timestamp": "2025-01-31T10:30:00",
  "stage": "extract",
  "status": "success",
  "records": 100,
  "duration_seconds": 2.5
}
```

## ğŸ”„ Agendamento

Para produÃ§Ã£o, use Apache Airflow ou cron:

```bash
# Exemplo cron: rodar diariamente Ã s 3am
0 3 * * * cd /path/to/project && python src/etl.py
```

## ğŸ› Troubleshooting

**Erro: "Connection refused"**
```bash
# Verificar se o PostgreSQL estÃ¡ rodando
docker ps | grep postgres

# Ver logs do container
docker logs etl-postgres
```

**Erro: "Duplicate key violation"**
```bash
# Limpar dados de staging e reprocessar
python src/etl.py --clean
```

## ğŸ“š PrÃ³ximos Passos

- [ ] Adicionar suporte a S3/GCS para staging
- [ ] Implementar Airflow DAG
- [ ] Adicionar dashboards com Metabase/Superset
- [ ] Testes de integraÃ§Ã£o com pytest
- [ ] CI/CD com GitHub Actions

## ğŸ› ï¸ Stack TecnolÃ³gica

- **Language:** Python 3.9+
- **Database:** PostgreSQL 14
- **Container:** Docker, Docker Compose
- **Libraries:** pandas, SQLAlchemy, psycopg2
- **Testing:** pytest

## ğŸ“„ LicenÃ§a

MIT License - use livremente para estudos e projetos.

## ğŸ¤ ContribuiÃ§Ãµes

Pull requests sÃ£o bem-vindos! Para grandes mudanÃ§as, abra uma issue primeiro.

## ğŸ“§ Contato

- GitHub: [@fleokleins-oss](https://github.com/fleokleins-oss)
- LinkedIn: [seu-perfil](https://linkedin.com/in/seu-perfil)

---

**Nota:** Este Ã© um projeto de demonstraÃ§Ã£o para portfolio. Em produÃ§Ã£o, considere:
- Secrets management (nÃ£o hardcode credenciais)
- Observability tools (DataDog, Prometheus)
- Data catalog (Apache Atlas, Amundsen)
- Testes mais abrangentes
